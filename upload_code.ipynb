{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "52b417bd-cc01-4cce-9e5d-522819fbcf31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mon Mar 11 10:51:17 2024       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 535.129.03             Driver Version: 535.129.03   CUDA Version: 12.2     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  NVIDIA GeForce RTX 4090        Off | 00000000:19:00.0 Off |                  Off |\n",
      "|  0%   29C    P8              32W / 450W |   1744MiB / 24564MiB |      0%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   1  NVIDIA GeForce RTX 4090        Off | 00000000:1A:00.0 Off |                  Off |\n",
      "|  0%   29C    P8              31W / 450W |    670MiB / 24564MiB |      0%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   2  NVIDIA GeForce RTX 4090        Off | 00000000:67:00.0 Off |                  Off |\n",
      "|  0%   30C    P8              30W / 450W |   2571MiB / 24564MiB |      0%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   3  NVIDIA GeForce RTX 4090        Off | 00000000:68:00.0 Off |                  Off |\n",
      "|  0%   29C    P8              37W / 450W |    866MiB / 24564MiB |      0%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "|    0   N/A  N/A      1222      G   /usr/lib/xorg/Xorg                            4MiB |\n",
      "|    0   N/A  N/A      2104      G   /usr/lib/xorg/Xorg                            4MiB |\n",
      "|    0   N/A  N/A      3147      G   /usr/lib/xorg/Xorg                            4MiB |\n",
      "|    0   N/A  N/A    189935      G   /usr/lib/xorg/Xorg                            4MiB |\n",
      "|    0   N/A  N/A   3985115      C   .../dp/anaconda3/envs/mitof/bin/python      614MiB |\n",
      "|    0   N/A  N/A   4032958      C   ...gbeen/anaconda3/envs/llm/bin/python     1014MiB |\n",
      "|    1   N/A  N/A      1222      G   /usr/lib/xorg/Xorg                            4MiB |\n",
      "|    1   N/A  N/A      2104      G   /usr/lib/xorg/Xorg                            4MiB |\n",
      "|    1   N/A  N/A      3147      G   /usr/lib/xorg/Xorg                            4MiB |\n",
      "|    1   N/A  N/A    189935      G   /usr/lib/xorg/Xorg                            4MiB |\n",
      "|    1   N/A  N/A   3937791      C   ...aconda3/envs/development/bin/python      614MiB |\n",
      "|    2   N/A  N/A      1222      G   /usr/lib/xorg/Xorg                            4MiB |\n",
      "|    2   N/A  N/A      2104      G   /usr/lib/xorg/Xorg                            4MiB |\n",
      "|    2   N/A  N/A      3147      G   /usr/lib/xorg/Xorg                            4MiB |\n",
      "|    2   N/A  N/A    189935      G   /usr/lib/xorg/Xorg                            4MiB |\n",
      "|    2   N/A  N/A   3937879      C   ...aconda3/envs/development/bin/python      614MiB |\n",
      "|    2   N/A  N/A   3983227      C   .../dp/anaconda3/envs/mitof/bin/python      614MiB |\n",
      "|    2   N/A  N/A   3985113      C   .../dp/anaconda3/envs/mitof/bin/python      614MiB |\n",
      "|    2   N/A  N/A   3985137      C   .../dp/anaconda3/envs/mitof/bin/python      614MiB |\n",
      "|    3   N/A  N/A      1222      G   /usr/lib/xorg/Xorg                           35MiB |\n",
      "|    3   N/A  N/A      2104      G   /usr/lib/xorg/Xorg                           35MiB |\n",
      "|    3   N/A  N/A      3147      G   /usr/lib/xorg/Xorg                           35MiB |\n",
      "|    3   N/A  N/A    189935      G   /usr/lib/xorg/Xorg                           79MiB |\n",
      "|    3   N/A  N/A    190102      G   /usr/bin/gnome-shell                        203MiB |\n",
      "+---------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09cde8e4-d4ab-4ff7-aab3-0aeb101d4641",
   "metadata": {},
   "source": [
    "### Library Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d6dcb0d4-0fc0-4729-8ec8-fd7eb8a6e8eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-03-11 10:51:24,208] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import pandas as pd\n",
    "import json\n",
    "import random\n",
    "import transformers\n",
    "import torch\n",
    "\n",
    "from tqdm.auto import notebook_tqdm\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, TrainingArguments, StoppingCriteria, StoppingCriteriaList\n",
    "from peft import LoraConfig, prepare_model_for_kbit_training, get_peft_model, PeftModel, PeftConfig\n",
    "from datasets import Dataset, load_dataset\n",
    "from trl import SFTTrainer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e48c89be-86b7-4fb2-ba65-00495769986c",
   "metadata": {},
   "source": [
    "### Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0b6a3c02-1f49-4752-ba71-d17ab7a59de9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ca9ceea1e714225b734b21516719763",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done!\n"
     ]
    }
   ],
   "source": [
    "csv_file = './data/train.csv'\n",
    "data = pd.read_csv(csv_file)\n",
    "\n",
    "json_data = []\n",
    "\n",
    "for _, row in notebook_tqdm(data.iterrows()):\n",
    "    question_1 = row['질문_1']\n",
    "    question_2 = row['질문_2']\n",
    "    \n",
    "    for i in range(1, 6):\n",
    "        answer = row[f'답변_{i}']\n",
    "        json_data.append({\n",
    "            \"question\": question_1+\"\\n\"+question_2,\n",
    "            \"answer\": answer\n",
    "        })\n",
    "\n",
    "json_string = json.dumps(json_data, ensure_ascii=False, indent=4)\n",
    "with open('train_json.json', 'w', encoding='utf-8') as file:\n",
    "    file.write(json_string)\n",
    "\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f485f05b-e459-472d-8ac5-f23ea16a6dce",
   "metadata": {},
   "source": [
    "### Augmented Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "87fbfe4e-4469-4c23-a969-6d1a83efac84",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./train_json.json', 'r', encoding='utf-8') as file:\n",
    "    data = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "10fa2774-86ea-4a28-957e-382a622d4384",
   "metadata": {},
   "outputs": [],
   "source": [
    "def augment_data(data_list):\n",
    "    # 데이터를 랜덤하게 선택\n",
    "    pair = random.sample(data_list, 2)\n",
    "\n",
    "    # 연결한 문구를 랜덤하게 선택\n",
    "    conjunctions = [\" 그리고 \", \" 또한 \", \" 과 \", \" 와 \"]\n",
    "    conjunction = random.choice(conjunctions)\n",
    "\n",
    "    # 두 데이터를 이어붙임\n",
    "    combined_question = pair[0]['question'] + conjunction + pair[1]['question']\n",
    "    combined_answer = pair[0]['answer'] + conjunction + pair[1]['answer']\n",
    "\n",
    "    new_data = {\n",
    "        \"question\": combined_question,\n",
    "        \"answer\": combined_answer\n",
    "    }\n",
    "    return new_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "daadcc35-26fc-4aec-b94c-aa20e377652e",
   "metadata": {},
   "outputs": [],
   "source": [
    "augmented_data = [augment_data(data) for _ in range(500)]\n",
    "augmented_full_data = data + augmented_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d349829a-e881-47da-85fd-cfc9323035d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./augmented_data.json', 'w', encoding='utf-8') as file:\n",
    "    json.dump(augmented_full_data, file, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "720c2218-2d9f-4976-a4cb-1a24ad94f8e2",
   "metadata": {},
   "source": [
    "### Model Finetuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf5160fe-d11d-4d34-a69b-e84c2bbee9d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8027789568ff444d8e62cbabaa956dea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_id = \"beomi/llama-2-koen-13b\"\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.float16\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_id,\n",
    "    eos_token=\"<|endoftext|>\"\n",
    "    )\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True,\n",
    "    rope_scaling = {\"type\": \"dynamic\", \"factor\": 2}\n",
    ")\n",
    "\n",
    "model.config.use_cache=False\n",
    "model.config.pretraining_tp=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d24e20a-32cb-4c6d-b519-d86ecf169251",
   "metadata": {},
   "outputs": [],
   "source": [
    "def trans(x):\n",
    "    return {'text':f\"당신은 질문에 답변하는 역할을 하는 챗봇입니다. 사용자의 질문에 올바른 답변을 하세요.\\n### 질문: {x['question']}\\n### 답변: {x['answer']}<|endoftext|>\"}\n",
    "    \n",
    "data = Dataset.from_json('augmented_data.json')\n",
    "train_data = data.map(lambda x: trans(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e3452ec-8279-4630-b543-5d854763db9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.train()\n",
    "model.gradient_checkpointing_enable()\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "\n",
    "output_path = \"./result/llama2-13b/\"\n",
    "\n",
    "config = LoraConfig(\n",
    "    lora_alpha=256,\n",
    "    lora_dropout=0.05,\n",
    "    r=128,\n",
    "    target_modules=['v_proj', 'up_proj', 'down_proj', 'k_proj', 'o_proj', 'q_proj', 'gate_proj'],\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")\n",
    "\n",
    "train_params = TrainingArguments(\n",
    "    output_dir=output_path,\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=2,\n",
    "    gradient_accumulation_steps=1,\n",
    "    save_strategy=\"epoch\", \n",
    "    optim=\"paged_adamw_8bit\",\n",
    "    learning_rate=1e-4,\n",
    "    logging_steps=100,\n",
    "    weight_decay=0.01,\n",
    "    max_grad_norm=0.3,\n",
    "    warmup_ratio=0.1,\n",
    "    fp16=True,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    seed=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98c1759c-ff2b-4c12-af04-fce61dbb7e95",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=train_data,\n",
    "    peft_config=config,\n",
    "    dataset_text_field='text',\n",
    "    tokenizer=tokenizer,\n",
    "    args=train_params,\n",
    "    data_collator=transformers.DataCollatorForLanguageModeling(tokenizer, mlm=False)\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "trainer.model.save_pretrained(output_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8449cf4-06d3-422e-90f7-da75422aa79a",
   "metadata": {},
   "source": [
    "### Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "755032f6-c7ec-4570-900a-6a199be62e73",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"beomi/llama-2-koen-13b\"\n",
    "peft_path = './result/llama2-13b/'\n",
    "config = PeftConfig.from_pretrained(peft_path)\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type='nf4',\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id,\n",
    "                                         eos_token=\"<|endoftext|>\")\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id,\n",
    "                                            quantization_config=bnb_config,\n",
    "                                            device_map=\"auto\",\n",
    "                                            trust_remote_code=True,\n",
    "                                            rope_scaling = {\"type\": \"dynamic\", \"factor\": 2})\n",
    "model = PeftModel.from_pretrained(model, peft_path)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6456950c-736a-4ad5-8170-ecfba1bad904",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LocalStoppingCriteria(StoppingCriteria):\n",
    "\n",
    "    def __init__(self, tokenizer, stop_words=[]):\n",
    "        super().__init__()\n",
    "\n",
    "        stops = [tokenizer(stop_word, return_tensors='pt', add_special_tokens=False)['input_ids'].squeeze() for\n",
    "                 stop_word in stop_words]\n",
    "        print('stop_words', stop_words)\n",
    "        print('stop_words_ids', stops)\n",
    "        self.stop_words = stop_words\n",
    "        self.stops = [stop.cuda() for stop in stops]\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def _compare_token(self, input_ids):\n",
    "        for stop in self.stops:\n",
    "            if len(stop.size()) != 1:\n",
    "                continue\n",
    "            stop_len = len(stop)\n",
    "            if torch.all((stop == input_ids[0][-stop_len:])).item():\n",
    "                return True\n",
    "\n",
    "        return False\n",
    "\n",
    "    def _compare_decode(self, input_ids):\n",
    "        input_str = self.tokenizer.decode(input_ids[0])\n",
    "        for stop_word in self.stop_words:\n",
    "            if input_str.endswith(stop_word):\n",
    "                return True\n",
    "        return False\n",
    "\n",
    "    def __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor):\n",
    "        input_str = self.tokenizer.decode(input_ids[0])\n",
    "        for stop_word in self.stop_words:\n",
    "            if input_str.endswith(stop_word):\n",
    "                return True\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1976757-bfbb-4749-8b03-6b87fe59703e",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = [\"<|endoftext|>\", \"###\", \"</s>\"]\n",
    "stopping_criteria = StoppingCriteriaList([LocalStoppingCriteria(tokenizer=tokenizer, stop_words=stop_words)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65c5bd1c-f427-4f29-8da5-1fc1e59b96c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen(x):\n",
    "    q = f\"당신은 질문에 답변하는 역할을 하는 챗봇입니다. 사용자의 질문에 올바른 답변을 하세요.\\n### 질문: {x}\\n### 답변:\"\n",
    "\n",
    "    gened = model.generate(\n",
    "        **tokenizer(\n",
    "            q, \n",
    "            return_tensors='pt',\n",
    "        ),\n",
    "        do_sample=True,\n",
    "        temperature=1.0,\n",
    "        num_beams=3,\n",
    "        top_p=1.0,\n",
    "        top_k=20,\n",
    "        epsilon_cutoff=9e-4,\n",
    "        eta_cutoff=2e-3,\n",
    "        penalty_alpha=1.0,\n",
    "        max_new_tokens=1024,\n",
    "        early_stopping=True,\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "        repetition_penalty=1.2,\n",
    "        stopping_criteria=stopping_criteria\n",
    "    )\n",
    "    \n",
    "    translation = tokenizer.decode(gened[0], skip_special_tokens=True)\n",
    "    translation = translation.split(\"### 답변:\")[1].split(\"<|endoftext|>\")[0].strip()\n",
    "    print('-------------------------------------------------------------------------------')\n",
    "    print(\"Question:\",x)\n",
    "    print('-------------------------------------------------------------------------------')\n",
    "    print(\"Answer:\",translation)\n",
    "    print(\"\")\n",
    "    return translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0abd1416-397e-4627-aa0b-58adebbe9932",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = pd.read_csv('./data/test.csv')\n",
    "test_df.head()\n",
    "\n",
    "temp_df = pd.DataFrame(columns=['answer'])\n",
    "temp_list = []\n",
    "\n",
    "for i in tqdm(range(len(test_df))):\n",
    "    temp_list.append(gen(test_df['질문'][i]))\n",
    "\n",
    "temp_df['answer'] = temp_list\n",
    "temp_df.to_csv('answer.csv', index=False, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ddf380c-de4c-4653-8f94-2addbaf363a7",
   "metadata": {},
   "source": [
    "### Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d83f721b-809d-49b6-919b-7ad9990daf99",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = pd.read_csv('answer.csv', encoding='utf-8')\n",
    "submission_df = pd.read_csv('./data/sample_submission.csv')\n",
    "\n",
    "model = SentenceTransformer('distiluse-base-multilingual-cased-v1')\n",
    "\n",
    "for i in tqdm(range(len(test_data))):\n",
    "    pred_embed = model.encode(test_data['answer'][i])\n",
    "    submission_df.loc[i, submission_df.columns[1:]] = pred_embed\n",
    "\n",
    "submission_df.to_csv('submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6b468b8-e520-46cd-af72-822c664c496a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
